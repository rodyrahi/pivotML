{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9V_9RChmQGs7"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeffheaton/present/blob/master/youtube/automl/simple-automl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "\n",
    "# Simple AutoML\n",
    "\n",
    "Copyright 2023 by [Jeff Heaton](https://youtube.com/@HeatonResearch), LGPL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "htVks4vk_BPL"
   },
   "outputs": [],
   "source": [
    "#DATA_SOURCE = \"https://data.heatonresearch.com/data/t81-558/auto-mpg.csv\"; TARGET = \"mpg\";IS_REGRESSION=True\n",
    "#DATA_SOURCE = \"https://data.heatonresearch.com/data/t81-558/iris.csv\"; TARGET = \"species\";IS_REGRESSION=False\n",
    "DATA_SOURCE = \"https://data.heatonresearch.com/data/t81-558/crx.csv\"; TARGET = 'a16'; IS_REGRESSION=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z21pOD98_kQc"
   },
   "source": [
    "# Analyze Data\n",
    "\n",
    "The following code implements the ```analyze``` function that determines the types and encodings of all columns in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "DSCFz5EBU4DN"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import shapiro\n",
    "import pandas as pd\n",
    "\n",
    "CONFIG_MAX_DUMMY = \"max_dummy\"\n",
    "CONFIG_MAX_DUMMY_PCT = \"pct_dummy\"\n",
    "\n",
    "CONFIG = {\n",
    "  CONFIG_MAX_DUMMY: 1000,\n",
    "  CONFIG_MAX_DUMMY_PCT: 0.75\n",
    "}\n",
    "\n",
    "def isnumeric(datatype):\n",
    "  return datatype in [FIELD_TYPE_FLOAT,FIELD_TYPE_INT]\n",
    "\n",
    "FIELDS = \"fields\"\n",
    "FIELD_ACTION = \"action\"\n",
    "FIELD_ACTION_COPY = \"copy\"\n",
    "FIELD_ACTION_IGNORE = \"ignore\"\n",
    "FIELD_ACTION_ZSCORE = \"zscore\"\n",
    "FIELD_ACTION_NORMALIZE = \"normalize\"\n",
    "FIELD_ACTION_DUMMY = \"dummy\"\n",
    "FIELD_ACTION_TARGET = \"target\"\n",
    "FIELD_NAME = \"name\"\n",
    "FIELD_SUM = \"sum\"\n",
    "FIELD_TYPE = \"type\"\n",
    "FIELD_MEAN = \"mean\"\n",
    "FIELD_NUM = \"n\"\n",
    "FIELD_MISSING = \"missing\"\n",
    "FIELD_MIN = \"min\"\n",
    "FIELD_MAX = \"max\"\n",
    "FIELD_VAR = \"var\"\n",
    "FIELD_SD = \"sd\"\n",
    "FIELD_UNIQUE = \"unique\"\n",
    "FIELD_MEDIAN = \"median\"\n",
    "FIELD_MODE = \"mode\"\n",
    "FIELD_SHAPIRO_STAT = \"shapiro-stat\"\n",
    "FIELD_SHAPIRO_P = \"shapiro-p\"\n",
    "META_TARGET = \"target\"\n",
    "META_TYPE = \"type\"\n",
    "META_TYPE_BINARY_CLASSIFICATION = \"binary-classification\"\n",
    "META_TYPE_CLASSIFICATION = \"classification\"\n",
    "META_TYPE_REGRESSION = \"regression\"\n",
    "META_SOURCE = \"source\"\n",
    "META_POSITIVE_TOKEN = \"positive-token\"\n",
    "META_EARLY_STOP = \"early-stop\"\n",
    "\n",
    "FIELD_TYPE_FLOAT = \"float\"\n",
    "FIELD_TYPE_INT = \"int\"\n",
    "FIELD_TYPE_STR = \"str\"\n",
    "\n",
    "def find_positive(s):\n",
    "  s = set(s.str.upper().tolist())\n",
    "  if len(s) != 2: return None\n",
    "  if \"+\" in s and \"-\" in s: return \"+\"\n",
    "  if \"0\" in s and \"1\" in s: return \"1\"\n",
    "  if \"t\" in s and \"f\" in s: return \"t\"\n",
    "  if \"y\" in s and \"n\" in s: return \"y\"\n",
    "  if \"true\" in s and \"false\" in s: return \"true\"\n",
    "  if \"yes\" in s and \"no\" in s: return \"yes\"\n",
    "  if \"p\" in s and \"n\" in s: return \"p\"\n",
    "  if \"positive\" in s and \"negative\" in s: return \"positive\"\n",
    "  s = list(s)\n",
    "  s.sort()\n",
    "  return s[0]\n",
    "\n",
    "def analyze(data_source, target, is_regression=True):\n",
    "  df = pd.read_csv(data_source,na_values=['NA', '?'])\n",
    "\n",
    "  metadata = {\n",
    "      FIELDS: {},\n",
    "      META_TARGET: target,\n",
    "      META_SOURCE: data_source,\n",
    "      META_EARLY_STOP: True\n",
    "  }\n",
    "\n",
    "  fields = metadata[FIELDS]\n",
    "\n",
    "  for field_name,csv_type in zip(df.columns,df.dtypes):\n",
    "    #print(name,csv_type)\n",
    "    if \"float\" in csv_type.name:\n",
    "      dtype = FIELD_TYPE_FLOAT\n",
    "      action = FIELD_ACTION_COPY\n",
    "    elif \"int\" in csv_type.name:\n",
    "      dtype = FIELD_TYPE_INT\n",
    "      action = FIELD_ACTION_COPY\n",
    "    else:\n",
    "      dtype = FIELD_TYPE_STR\n",
    "      action = FIELD_ACTION_IGNORE\n",
    "\n",
    "    missing_count = sum(df[field_name].isnull())\n",
    "    col = df[field_name]\n",
    "    unique_count = len(pd.unique(col))\n",
    "\n",
    "    if isnumeric(dtype):\n",
    "      stat, p = shapiro(col)\n",
    "\n",
    "      # less than or equal to 0.05 not normal\n",
    "      action = FIELD_ACTION_ZSCORE if p>0.05 else FIELD_ACTION_NORMALIZE\n",
    "\n",
    "      fields[field_name] = {\n",
    "          FIELD_TYPE:dtype,\n",
    "          FIELD_MEDIAN:col.median(),\n",
    "          FIELD_MEAN:col.mean(),\n",
    "          FIELD_SD:col.std(),\n",
    "          FIELD_MAX:col.max(),\n",
    "          FIELD_MIN:col.min(),\n",
    "          FIELD_SHAPIRO_STAT:stat,\n",
    "          FIELD_SHAPIRO_P:p,\n",
    "          FIELD_ACTION:action,\n",
    "          FIELD_MISSING:missing_count,\n",
    "          FIELD_UNIQUE:unique_count}\n",
    "\n",
    "    else:\n",
    "      fields[field_name] = {\n",
    "          FIELD_TYPE:dtype,\n",
    "          FIELD_MODE:col.mode()[0],\n",
    "          FIELD_ACTION:action,\n",
    "          FIELD_MISSING:missing_count,\n",
    "          FIELD_UNIQUE:unique_count}\n",
    "\n",
    "    # Determine action\n",
    "    field = fields[field_name]\n",
    "    if (field[FIELD_TYPE] == FIELD_TYPE_STR) and (field[FIELD_UNIQUE]<CONFIG[CONFIG_MAX_DUMMY]) and (field[FIELD_UNIQUE]/len(df)<CONFIG[CONFIG_MAX_DUMMY_PCT]):\n",
    "      field[FIELD_ACTION] = FIELD_ACTION_DUMMY\n",
    "    if field_name == target:\n",
    "      field[FIELD_ACTION] = FIELD_ACTION_TARGET\n",
    "  \n",
    "  # Determine model type\n",
    "  is_binary = (metadata[FIELDS][target][FIELD_UNIQUE]==2) and not is_regression\n",
    "\n",
    "  if is_regression:\n",
    "    metadata[META_TYPE] = META_TYPE_REGRESSION\n",
    "  else:\n",
    "    if metadata[FIELDS][target][FIELD_UNIQUE]==2:\n",
    "      metadata[META_TYPE] = META_TYPE_BINARY_CLASSIFICATION\n",
    "\n",
    "      metadata[META_POSITIVE_TOKEN] = find_positive(df[target])\n",
    "    else:\n",
    "      metadata[META_TYPE] = META_TYPE_CLASSIFICATION\n",
    "\n",
    "  return metadata\n",
    "\n",
    "COLS = [FIELD_MEAN, FIELD_SD, FIELD_MEDIAN, FIELD_MODE, FIELD_MAX, FIELD_ACTION, FIELD_UNIQUE, FIELD_SHAPIRO_P,FIELD_MISSING]\n",
    "\n",
    "def field_summary(metadata, cols=COLS):\n",
    "  data = {}\n",
    "\n",
    "  data['name'] = []\n",
    "  for col in cols:\n",
    "    data[col] = []\n",
    "\n",
    "  for field_name in metadata[FIELDS]:\n",
    "    field = metadata[FIELDS][field_name]\n",
    "    data['name'].append(field_name)\n",
    "    for col in cols:\n",
    "      data[col].append(field.get(col, None))\n",
    "\n",
    "  return pd.DataFrame(data)[['name']+COLS]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZOJ_jcYE234"
   },
   "source": [
    "# Generate Code\n",
    "\n",
    "The following code generates Keras Python code for the data that was analzyed in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "U3p2kabMhX1G"
   },
   "outputs": [],
   "source": [
    "from dataclasses import MISSING\n",
    "from pandas.core.dtypes.inference import is_re\n",
    "def tolist(obj):\n",
    "  if isinstance(obj,list) or isinstance(obj, tuple):\n",
    "    return obj\n",
    "  else:\n",
    "    return [obj]\n",
    "\n",
    "class PythonFile:\n",
    "  def __init__(self):\n",
    "    self.imports = []\n",
    "    self.lines = []\n",
    "\n",
    "  def add_import(self, name, alias=None):\n",
    "    if alias:\n",
    "      self.imports.append({\"name\": name, \"alias\": alias})\n",
    "    else:\n",
    "      self.imports.append({\"name\": name})\n",
    "\n",
    "  def add_from(self, _from, _import):\n",
    "    self.imports.append({\"from\": _from, \"import\": _import})\n",
    "    \n",
    "  def generate(self):\n",
    "    src = \"\"\n",
    "    for obj in self.imports:\n",
    "      if \"name\" in obj and \"alias\" in obj:\n",
    "        src += f\"import {obj['name']} as {obj['alias']}\"\n",
    "      elif \"name\" in obj and \"alias\" not in obj:\n",
    "        src += f\"import {obj['name']}\"\n",
    "      elif \"from\" in obj and \"import\" in obj:\n",
    "        imports = \", \".join(tolist(obj['import']))\n",
    "        src += f\"from {obj['from']} import {imports}\"\n",
    "\n",
    "      src+=\"\\n\"\n",
    "\n",
    "    for line in self.lines:\n",
    "      src+=line+\"\\n\"\n",
    "    return src\n",
    "\n",
    "  def add_line(self, str):\n",
    "    self.lines.append(str)\n",
    "\n",
    "  def comment(self, str):\n",
    "    return f\"# {str}\"\n",
    "\n",
    "  def call(self, name, *args):\n",
    "    src = name + \"(\"\n",
    "\n",
    "    formatted_args = []\n",
    "    started_named = False\n",
    "    for arg in args:\n",
    "      if isinstance(arg,dict):\n",
    "        formatted_args += [f\"{name}={arg[name]}\" for name in arg.keys()]\n",
    "        started_named = True\n",
    "      else: \n",
    "        if started_named: raise ValueError(\"positional argument follows keyword argument\")\n",
    "        formatted_args.append(str(arg))\n",
    "\n",
    "    src += \", \".join(formatted_args)\n",
    "    src += \")\"\n",
    "    return src\n",
    "\n",
    "  def assign(self, left, right):\n",
    "    return f\"{left} = {right}\"\n",
    "\n",
    "  def str(self, str):\n",
    "    return f\"\\\"{str}\\\"\"\n",
    "\n",
    "  def index(self, name, indexes, dot=None):\n",
    "    src = name\n",
    "    for idx in indexes:\n",
    "      src+=f'[{idx}]'\n",
    "\n",
    "    if dot:\n",
    "      src+='.'\n",
    "      src+=dot\n",
    "    return src\n",
    "    \n",
    "def generate_keras(metadata):\n",
    "  na_values = ['NA', '?']\n",
    "  target = metadata[META_TARGET]\n",
    "  is_regression = metadata[META_TYPE] == META_TYPE_REGRESSION\n",
    "  is_binary = (metadata[FIELDS][target][FIELD_UNIQUE]==2) and (metadata[META_TYPE]==META_TYPE_CLASSIFICATION)\n",
    "\n",
    "  if metadata[META_TYPE] == META_TYPE_REGRESSION:\n",
    "    loss = \"mean_squared_error\"\n",
    "  elif metadata[META_TYPE] == META_TYPE_BINARY_CLASSIFICATION:\n",
    "    loss = \"binary_crossentropy\"\n",
    "  else:\n",
    "    loss = \"categorical_crossentropy\"\n",
    "\n",
    "  py = PythonFile()\n",
    "  # Imports\n",
    "  py.add_import(\"pandas\", \"pd\")\n",
    "  py.add_import(\"io\")\n",
    "  py.add_import(\"requests\")\n",
    "  py.add_import(\"numpy\", \"np\")\n",
    "  py.add_from(\"tensorflow.keras.models\", \"Sequential\")\n",
    "  py.add_from(\"tensorflow.keras.layers\", [\"Dense\", \"Activation\"])\n",
    "  py.add_from(\"tensorflow.keras.callbacks\", \"EarlyStopping\")\n",
    "  py.add_from(\"scipy.stats\", \"zscore\")\n",
    "  py.add_from(\"sklearn.preprocessing\", \"MinMaxScaler\")\n",
    "\n",
    "  py.add_line(py.assign(\"df\", py.call(\"pd.read_csv\",py.str(metadata[META_SOURCE]),{'na_values':na_values})))\n",
    "  x_fields = [x for x in metadata[FIELDS] if x != target and metadata[FIELDS][x][FIELD_ACTION] in [FIELD_ACTION_COPY]]\n",
    "  py.add_line(py.assign(\"x_fields\",x_fields))\n",
    "  \n",
    "  # Analyze input columns\n",
    "  for field_name in metadata[FIELDS]:\n",
    "    field = metadata[FIELDS][field_name]\n",
    "    if field[FIELD_MISSING]>0:\n",
    "      if isnumeric(field[FIELD_TYPE]):\n",
    "        fn = \"median\"\n",
    "        suffix = \"\"\n",
    "      else:\n",
    "        fn = \"mode\"\n",
    "        suffix = \"[0]\"\n",
    "      py.add_line(py.assign(py.index(\"df\",[py.str(field_name)]),\n",
    "                py.index(\"df\",[py.str(field_name)],py.call(\"fillna\",\n",
    "                py.index(\"df\",[py.str(field_name)],py.call(fn)+suffix)\n",
    "                ))))\n",
    "    if field[FIELD_ACTION] == FIELD_ACTION_ZSCORE:\n",
    "      py.add_line(py.assign(py.index(\"df\",[py.str(field_name)]),\n",
    "                py.call(\"zscore\",py.index(\"df\",[py.str(field_name)]))))\n",
    "      py.add_line(py.call(\"x_fields.append\",py.str(field_name)))\n",
    "    elif field[FIELD_ACTION] == FIELD_ACTION_NORMALIZE:\n",
    "      f1 = py.index(\"df\",[py.str(field_name)])\n",
    "      f2 = py.index(\"df\",[[field_name]])\n",
    "      py.add_line(py.assign(f1,py.call(\"MinMaxScaler().fit_transform\",f2)))\n",
    "      py.add_line(py.call(\"x_fields.append\",py.str(field_name)))\n",
    "    elif field[FIELD_ACTION] == FIELD_ACTION_DUMMY:\n",
    "      py.add_line(py.assign(\"dummies\", \n",
    "            py.call(\"pd.get_dummies\",\n",
    "            py.index('df',[py.str(field_name)]),\n",
    "            {'prefix':py.str(field_name),'drop_first':'True'})))\n",
    "      py.add_line(\"df = pd.concat([df,dummies],axis=1)\")\n",
    "      py.add_line(\"x_fields += dummies.columns.tolist()\")\n",
    "      \n",
    "\n",
    "\n",
    "  py.add_line(py.assign(\"x\",py.index(\"df\",[\"x_fields\"],\"values\")))\n",
    "\n",
    "  if metadata[META_TYPE] == META_TYPE_CLASSIFICATION:\n",
    "    py.add_line(py.assign(\"dummies\", py.call(\"pd.get_dummies\", py.index(\"df\", [py.str(target)]))))\n",
    "    py.add_line(py.assign(\"species\", \"dummies.columns\"))\n",
    "    py.add_line(py.assign(\"y\", \"dummies.values\"))\n",
    "  elif metadata[META_TYPE] == META_TYPE_BINARY_CLASSIFICATION:\n",
    "    t = py.index(\"df\",[py.str(target)])\n",
    "    pos = metadata[META_POSITIVE_TOKEN]\n",
    "    py.add_line(py.assign(t,f\"({t}=={py.str(pos)}).astype(int)\"))\n",
    "    py.add_line(py.assign(\"y\", f\"df.{target}.values\"))\n",
    "  else:\n",
    "    py.add_line(py.assign(\"y\", f\"df.{target}.values\"))\n",
    "\n",
    "  py.add_line(py.comment(\"Construct model\"))\n",
    "  # Early stop\n",
    "  if metadata[META_EARLY_STOP]:\n",
    "    x_train, y_train, x_test, y_test = \"x_train\", \"y_train\", \"x_test\", \"y_test\"\n",
    "    py.add_from(\"sklearn.model_selection\", \"train_test_split\")\n",
    "    py.add_line(py.comment(\"Split into validation and training sets\"))\n",
    "    py.add_line(py.assign(f\"{x_train}, {x_test}, {y_train}, {y_test}\",\n",
    "        py.call(\"train_test_split\",\"x\",\"y\",{\"test_size\":0.25,\"random_state\":42})))\n",
    "  else:\n",
    "    x_train, y_train, x_test, y_test = \"x\", \"y\", \"x\", \"y\"\n",
    "\n",
    "  py.add_line(py.assign(\"model\",py.call(\"Sequential\")))\n",
    "  py.add_line(py.call(\"model.add\", py.call(\"Dense\",50,{\"input_dim\":\"x.shape[1]\", \"activation\":py.str('relu')})))\n",
    "  py.add_line(py.call(\"model.add\", py.call(\"Dense\",25,{\"activation\":py.str('relu')})))\n",
    "  if metadata[META_TYPE] == META_TYPE_REGRESSION:\n",
    "    py.add_line(py.call(\"model.add\", py.call(\"Dense\",\"1\")))\n",
    "  elif metadata[META_TYPE] == META_TYPE_BINARY_CLASSIFICATION:\n",
    "    py.add_line(py.call(\"model.add\", py.call(\"Dense\",\"1\",{\"activation\":py.str('sigmoid')})))\n",
    "  else:  \n",
    "    py.add_line(py.call(\"model.add\", py.call(\"Dense\",\"y.shape[1]\",{\"activation\":py.str('softmax')})))\n",
    "  py.add_line(py.call(\"model.compile\", {\"loss\":py.str(loss), \"optimizer\":py.str('adam')}))\n",
    "\n",
    "  py.add_line(py.comment(\"Train model\"))\n",
    "  if metadata[META_EARLY_STOP]:\n",
    "    py.add_line(py.assign(\"monitor\",py.call(\"EarlyStopping\",{\"monitor\":py.str('val_loss'), \"min_delta\":\"1e-3\", \"patience\":5, \n",
    "        \"verbose\":1, \"mode\":py.str('auto'), \"restore_best_weights\":True})))\n",
    "    py.add_line(py.call(\"model.fit\", x_train, y_train, \n",
    "        {\"validation_data\": f\"({x_test},{y_test})\", 'callbacks':'[monitor]', 'verbose':'2','epochs':1000}))\n",
    "  else:\n",
    "    py.add_line(py.call(\"model.fit\", x_train, y_train, {'verbose':'2','epochs':100}))\n",
    "  \n",
    "  py.add_line(py.comment(\"Evaluate model\"))\n",
    "  py.add_from(\"sklearn\", \"metrics\")\n",
    "  py.add_line(py.assign(\"pred\", py.call(\"model.predict\", x_test)))\n",
    "  if metadata[META_TYPE] == META_TYPE_REGRESSION:\n",
    "    py.add_line(py.comment(\"Measure RMSE error.  RMSE is common for regression.\"))\n",
    "    py.add_line(py.assign(\"score\", py.call(\"np.sqrt\", py.call(\"metrics.mean_squared_error\", \"pred\", y_test))))\n",
    "    py.add_line(\"print(f\\\"Root mean square (RMSE): {score}\\\")\")\n",
    "  if metadata[META_TYPE] == META_TYPE_CLASSIFICATION:\n",
    "    py.add_line(py.assign(\"predict_classes\", py.call(\"np.argmax\", \"pred\", {\"axis\":1})))\n",
    "    py.add_line(py.assign(\"expected_classes\", py.call(\"np.argmax\", y_test, {\"axis\":1})))\n",
    "    py.add_line(py.assign(\"correct\", py.call(\"accuracy_score\", \"expected_classes\", \"predict_classes\")))\n",
    "    py.add_line(\"print(f\\\"Accuracy: {correct}\\\")\")\n",
    "  elif metadata[META_TYPE] == META_TYPE_BINARY_CLASSIFICATION :\n",
    "    py.add_line(py.assign(\"predict_classes\", py.call(\"np.argmax\", \"pred\", {\"axis\":1})))\n",
    "    py.add_line(py.assign(\"correct\", py.call(\"accuracy_score\", y_test, \"predict_classes\")))\n",
    "    py.add_line(\"print(f\\\"Accuracy: {correct}\\\")\")\n",
    "    py.add_line(py.assign(\"fpr, tpr, thresholds\", py.call(\"metrics.roc_curve\", y_test, \"pred\", {\"pos_label\":1})))\n",
    "    py.add_line(py.assign(\"score\",py.call(\"metrics.auc\", \"fpr\", \"tpr\")))\n",
    "    py.add_line(\"print(f\\\"Area Under Curve: {score}\\\")\")\n",
    "  if metadata[META_TYPE] == META_TYPE_CLASSIFICATION or metadata[META_TYPE] == META_TYPE_BINARY_CLASSIFICATION :\n",
    "    py.add_from(\"sklearn.metrics\", \"accuracy_score\")\n",
    "    py.add_line(py.assign(\"score\", py.call(\"metrics.log_loss\", y_test, \"pred\", {'eps': 1e-7})))\n",
    "    py.add_line(\"print(f\\\"Log loss: {score}\\\")\")\n",
    "\n",
    "  return py.generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EbDxHCJoR-a5"
   },
   "source": [
    "# Running the AutoML Generator\n",
    "\n",
    "We begin by analyzing the dataset specified at the top of this notebook. We display the summary statustics on the dataset. You can change the \"action\" for any of these objects if you do not like the preprocessing action detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 589
    },
    "id": "_mQ_Ovee_1NM",
    "outputId": "3873b101-50b2-4e91-e1a0-2470b57c4fac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fields': {'a1': {'type': 'str', 'mode': 'b', 'action': 'dummy', 'missing': 12, 'unique': 3}, 'a2': {'type': 'float', 'median': 28.46, 'mean': 31.56817109144543, 'sd': 11.957862498270877, 'max': 80.25, 'min': 13.75, 'shapiro-stat': nan, 'shapiro-p': nan, 'action': 'normalize', 'missing': 12, 'unique': 350}, 's3': {'type': 'float', 'median': 2.75, 'mean': 4.758724637681159, 'sd': 4.978163248528541, 'max': 28.0, 'min': 0.0, 'shapiro-stat': 0.8302546989708606, 'shapiro-p': 2.009391244299927e-26, 'action': 'normalize', 'missing': 0, 'unique': 215}, 'a4': {'type': 'str', 'mode': 'u', 'action': 'dummy', 'missing': 6, 'unique': 4}, 'a5': {'type': 'str', 'mode': 'g', 'action': 'dummy', 'missing': 6, 'unique': 4}, 'a6': {'type': 'str', 'mode': 'c', 'action': 'dummy', 'missing': 9, 'unique': 15}, 'a7': {'type': 'str', 'mode': 'v', 'action': 'dummy', 'missing': 9, 'unique': 10}, 'a8': {'type': 'float', 'median': 1.0, 'mean': 2.223405797101449, 'sd': 3.3465133592781324, 'max': 28.5, 'min': 0.0, 'shapiro-stat': 0.6652944554887558, 'shapiro-p': 1.1096840819543242e-34, 'action': 'normalize', 'missing': 0, 'unique': 132}, 'a9': {'type': 'str', 'mode': 't', 'action': 'dummy', 'missing': 0, 'unique': 2}, 'a10': {'type': 'str', 'mode': 'f', 'action': 'dummy', 'missing': 0, 'unique': 2}, 'a11': {'type': 'int', 'median': 0.0, 'mean': 2.4, 'sd': 4.862940034226996, 'max': 67, 'min': 0, 'shapiro-stat': 0.5330627647585819, 'shapiro-p': 3.662317517782372e-39, 'action': 'normalize', 'missing': 0, 'unique': 23}, 'a12': {'type': 'str', 'mode': 'f', 'action': 'dummy', 'missing': 0, 'unique': 2}, 'a13': {'type': 'str', 'mode': 'g', 'action': 'dummy', 'missing': 0, 'unique': 3}, 'a14': {'type': 'float', 'median': 160.0, 'mean': 184.01477104874445, 'sd': 173.80676822523813, 'max': 2000.0, 'min': 0.0, 'shapiro-stat': nan, 'shapiro-p': nan, 'action': 'normalize', 'missing': 13, 'unique': 171}, 'a15': {'type': 'int', 'median': 5.0, 'mean': 1017.3855072463768, 'sd': 5210.10259830269, 'max': 100000, 'min': 0, 'shapiro-stat': 0.16985436004252175, 'shapiro-p': 1.4053433496862405e-47, 'action': 'normalize', 'missing': 0, 'unique': 240}, 'a16': {'type': 'str', 'mode': '-', 'action': 'target', 'missing': 0, 'unique': 2}}, 'target': 'a16', 'source': 'https://data.heatonresearch.com/data/t81-558/crx.csv', 'early-stop': True, 'type': 'binary-classification', 'positive-token': '+'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "mean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sd",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "median",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mode",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "max",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "action",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "unique",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "shapiro-p",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "missing",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "077c7831-6934-49a5-883b-7fd931bb72e3",
       "rows": [
        [
         "0",
         "a1",
         null,
         null,
         null,
         "b",
         null,
         "dummy",
         "3",
         null,
         "12"
        ],
        [
         "1",
         "a2",
         "31.56817109144543",
         "11.957862498270877",
         "28.46",
         null,
         "80.25",
         "normalize",
         "350",
         null,
         "12"
        ],
        [
         "2",
         "s3",
         "4.758724637681159",
         "4.978163248528541",
         "2.75",
         null,
         "28.0",
         "normalize",
         "215",
         "2.009391244299927e-26",
         "0"
        ],
        [
         "3",
         "a4",
         null,
         null,
         null,
         "u",
         null,
         "dummy",
         "4",
         null,
         "6"
        ],
        [
         "4",
         "a5",
         null,
         null,
         null,
         "g",
         null,
         "dummy",
         "4",
         null,
         "6"
        ],
        [
         "5",
         "a6",
         null,
         null,
         null,
         "c",
         null,
         "dummy",
         "15",
         null,
         "9"
        ],
        [
         "6",
         "a7",
         null,
         null,
         null,
         "v",
         null,
         "dummy",
         "10",
         null,
         "9"
        ],
        [
         "7",
         "a8",
         "2.223405797101449",
         "3.3465133592781324",
         "1.0",
         null,
         "28.5",
         "normalize",
         "132",
         "1.1096840819543242e-34",
         "0"
        ],
        [
         "8",
         "a9",
         null,
         null,
         null,
         "t",
         null,
         "dummy",
         "2",
         null,
         "0"
        ],
        [
         "9",
         "a10",
         null,
         null,
         null,
         "f",
         null,
         "dummy",
         "2",
         null,
         "0"
        ],
        [
         "10",
         "a11",
         "2.4",
         "4.862940034226996",
         "0.0",
         null,
         "67.0",
         "normalize",
         "23",
         "3.662317517782372e-39",
         "0"
        ],
        [
         "11",
         "a12",
         null,
         null,
         null,
         "f",
         null,
         "dummy",
         "2",
         null,
         "0"
        ],
        [
         "12",
         "a13",
         null,
         null,
         null,
         "g",
         null,
         "dummy",
         "3",
         null,
         "0"
        ],
        [
         "13",
         "a14",
         "184.01477104874445",
         "173.80676822523813",
         "160.0",
         null,
         "2000.0",
         "normalize",
         "171",
         null,
         "13"
        ],
        [
         "14",
         "a15",
         "1017.3855072463768",
         "5210.10259830269",
         "5.0",
         null,
         "100000.0",
         "normalize",
         "240",
         "1.4053433496862405e-47",
         "0"
        ],
        [
         "15",
         "a16",
         null,
         null,
         null,
         "-",
         null,
         "target",
         "2",
         null,
         "0"
        ]
       ],
       "shape": {
        "columns": 10,
        "rows": 16
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>median</th>\n",
       "      <th>mode</th>\n",
       "      <th>max</th>\n",
       "      <th>action</th>\n",
       "      <th>unique</th>\n",
       "      <th>shapiro-p</th>\n",
       "      <th>missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dummy</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a2</td>\n",
       "      <td>31.568171</td>\n",
       "      <td>11.957862</td>\n",
       "      <td>28.46</td>\n",
       "      <td>None</td>\n",
       "      <td>80.25</td>\n",
       "      <td>normalize</td>\n",
       "      <td>350</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s3</td>\n",
       "      <td>4.758725</td>\n",
       "      <td>4.978163</td>\n",
       "      <td>2.75</td>\n",
       "      <td>None</td>\n",
       "      <td>28.00</td>\n",
       "      <td>normalize</td>\n",
       "      <td>215</td>\n",
       "      <td>2.009391e-26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dummy</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>g</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dummy</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>a6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dummy</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>a7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>v</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dummy</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>a8</td>\n",
       "      <td>2.223406</td>\n",
       "      <td>3.346513</td>\n",
       "      <td>1.00</td>\n",
       "      <td>None</td>\n",
       "      <td>28.50</td>\n",
       "      <td>normalize</td>\n",
       "      <td>132</td>\n",
       "      <td>1.109684e-34</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>a9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>t</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dummy</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>a10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dummy</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>a11</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>4.862940</td>\n",
       "      <td>0.00</td>\n",
       "      <td>None</td>\n",
       "      <td>67.00</td>\n",
       "      <td>normalize</td>\n",
       "      <td>23</td>\n",
       "      <td>3.662318e-39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>a12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dummy</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>a13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>g</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dummy</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>a14</td>\n",
       "      <td>184.014771</td>\n",
       "      <td>173.806768</td>\n",
       "      <td>160.00</td>\n",
       "      <td>None</td>\n",
       "      <td>2000.00</td>\n",
       "      <td>normalize</td>\n",
       "      <td>171</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>a15</td>\n",
       "      <td>1017.385507</td>\n",
       "      <td>5210.102598</td>\n",
       "      <td>5.00</td>\n",
       "      <td>None</td>\n",
       "      <td>100000.00</td>\n",
       "      <td>normalize</td>\n",
       "      <td>240</td>\n",
       "      <td>1.405343e-47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>a16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>target</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   name         mean           sd  median  mode        max     action  unique  \\\n",
       "0    a1          NaN          NaN     NaN     b        NaN      dummy       3   \n",
       "1    a2    31.568171    11.957862   28.46  None      80.25  normalize     350   \n",
       "2    s3     4.758725     4.978163    2.75  None      28.00  normalize     215   \n",
       "3    a4          NaN          NaN     NaN     u        NaN      dummy       4   \n",
       "4    a5          NaN          NaN     NaN     g        NaN      dummy       4   \n",
       "5    a6          NaN          NaN     NaN     c        NaN      dummy      15   \n",
       "6    a7          NaN          NaN     NaN     v        NaN      dummy      10   \n",
       "7    a8     2.223406     3.346513    1.00  None      28.50  normalize     132   \n",
       "8    a9          NaN          NaN     NaN     t        NaN      dummy       2   \n",
       "9   a10          NaN          NaN     NaN     f        NaN      dummy       2   \n",
       "10  a11     2.400000     4.862940    0.00  None      67.00  normalize      23   \n",
       "11  a12          NaN          NaN     NaN     f        NaN      dummy       2   \n",
       "12  a13          NaN          NaN     NaN     g        NaN      dummy       3   \n",
       "13  a14   184.014771   173.806768  160.00  None    2000.00  normalize     171   \n",
       "14  a15  1017.385507  5210.102598    5.00  None  100000.00  normalize     240   \n",
       "15  a16          NaN          NaN     NaN     -        NaN     target       2   \n",
       "\n",
       "       shapiro-p  missing  \n",
       "0            NaN       12  \n",
       "1            NaN       12  \n",
       "2   2.009391e-26        0  \n",
       "3            NaN        6  \n",
       "4            NaN        6  \n",
       "5            NaN        9  \n",
       "6            NaN        9  \n",
       "7   1.109684e-34        0  \n",
       "8            NaN        0  \n",
       "9            NaN        0  \n",
       "10  3.662318e-39        0  \n",
       "11           NaN        0  \n",
       "12           NaN        0  \n",
       "13           NaN       13  \n",
       "14  1.405343e-47        0  \n",
       "15           NaN        0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metadata = analyze(DATA_SOURCE, TARGET, IS_REGRESSION)\n",
    "print(metadata)\n",
    "summary = field_summary(metadata)\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gsbX5T3ISEpQ"
   },
   "source": [
    "Next we generate the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pCjxl-tkABRk",
    "outputId": "baa280ea-8d32-46db-f910-2f91ff2010ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pandas as pd\n",
      "import io\n",
      "import requests\n",
      "import numpy as np\n",
      "from tensorflow.keras.models import Sequential\n",
      "from tensorflow.keras.layers import Dense, Activation\n",
      "from tensorflow.keras.callbacks import EarlyStopping\n",
      "from scipy.stats import zscore\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "from sklearn import metrics\n",
      "from sklearn.metrics import accuracy_score\n",
      "df = pd.read_csv(\"https://data.heatonresearch.com/data/t81-558/crx.csv\", na_values=['NA', '?'])\n",
      "x_fields = []\n",
      "df[\"a1\"] = df[\"a1\"].fillna(df[\"a1\"].mode()[0])\n",
      "dummies = pd.get_dummies(df[\"a1\"], prefix=\"a1\", drop_first=True)\n",
      "df = pd.concat([df,dummies],axis=1)\n",
      "x_fields += dummies.columns.tolist()\n",
      "df[\"a2\"] = df[\"a2\"].fillna(df[\"a2\"].median())\n",
      "df[\"a2\"] = MinMaxScaler().fit_transform(df[['a2']])\n",
      "x_fields.append(\"a2\")\n",
      "df[\"s3\"] = MinMaxScaler().fit_transform(df[['s3']])\n",
      "x_fields.append(\"s3\")\n",
      "df[\"a4\"] = df[\"a4\"].fillna(df[\"a4\"].mode()[0])\n",
      "dummies = pd.get_dummies(df[\"a4\"], prefix=\"a4\", drop_first=True)\n",
      "df = pd.concat([df,dummies],axis=1)\n",
      "x_fields += dummies.columns.tolist()\n",
      "df[\"a5\"] = df[\"a5\"].fillna(df[\"a5\"].mode()[0])\n",
      "dummies = pd.get_dummies(df[\"a5\"], prefix=\"a5\", drop_first=True)\n",
      "df = pd.concat([df,dummies],axis=1)\n",
      "x_fields += dummies.columns.tolist()\n",
      "df[\"a6\"] = df[\"a6\"].fillna(df[\"a6\"].mode()[0])\n",
      "dummies = pd.get_dummies(df[\"a6\"], prefix=\"a6\", drop_first=True)\n",
      "df = pd.concat([df,dummies],axis=1)\n",
      "x_fields += dummies.columns.tolist()\n",
      "df[\"a7\"] = df[\"a7\"].fillna(df[\"a7\"].mode()[0])\n",
      "dummies = pd.get_dummies(df[\"a7\"], prefix=\"a7\", drop_first=True)\n",
      "df = pd.concat([df,dummies],axis=1)\n",
      "x_fields += dummies.columns.tolist()\n",
      "df[\"a8\"] = MinMaxScaler().fit_transform(df[['a8']])\n",
      "x_fields.append(\"a8\")\n",
      "dummies = pd.get_dummies(df[\"a9\"], prefix=\"a9\", drop_first=True)\n",
      "df = pd.concat([df,dummies],axis=1)\n",
      "x_fields += dummies.columns.tolist()\n",
      "dummies = pd.get_dummies(df[\"a10\"], prefix=\"a10\", drop_first=True)\n",
      "df = pd.concat([df,dummies],axis=1)\n",
      "x_fields += dummies.columns.tolist()\n",
      "df[\"a11\"] = MinMaxScaler().fit_transform(df[['a11']])\n",
      "x_fields.append(\"a11\")\n",
      "dummies = pd.get_dummies(df[\"a12\"], prefix=\"a12\", drop_first=True)\n",
      "df = pd.concat([df,dummies],axis=1)\n",
      "x_fields += dummies.columns.tolist()\n",
      "dummies = pd.get_dummies(df[\"a13\"], prefix=\"a13\", drop_first=True)\n",
      "df = pd.concat([df,dummies],axis=1)\n",
      "x_fields += dummies.columns.tolist()\n",
      "df[\"a14\"] = df[\"a14\"].fillna(df[\"a14\"].median())\n",
      "df[\"a14\"] = MinMaxScaler().fit_transform(df[['a14']])\n",
      "x_fields.append(\"a14\")\n",
      "df[\"a15\"] = MinMaxScaler().fit_transform(df[['a15']])\n",
      "x_fields.append(\"a15\")\n",
      "x = df[x_fields].values\n",
      "df[\"a16\"] = (df[\"a16\"]==\"+\").astype(int)\n",
      "y = df.a16.values\n",
      "# Construct model\n",
      "model = Sequential()\n",
      "model.add(Dense(50, input_dim=x.shape[1], activation=\"relu\"))\n",
      "model.add(Dense(25, activation=\"relu\"))\n",
      "model.add(Dense(1, activation=\"sigmoid\"))\n",
      "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
      "# Train model\n",
      "model.fit(x, y, verbose=2, epochs=100)\n",
      "# Evaluate model\n",
      "pred = model.predict(x)\n",
      "predict_classes = np.argmax(pred, axis=1)\n",
      "correct = accuracy_score(y, predict_classes)\n",
      "print(f\"Accuracy: {correct}\")\n",
      "fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=1)\n",
      "score = metrics.auc(fpr, tpr)\n",
      "print(f\"Area Under Curve: {score}\")\n",
      "score = metrics.log_loss(y, pred, eps=1e-07)\n",
      "print(f\"Log loss: {score}\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metadata[META_EARLY_STOP] = False\n",
    "python_code = generate_keras(metadata)\n",
    "print(python_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Simple-Auto-ML\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid dtype: object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 70\u001b[0m\n\u001b[0;32m     68\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Evaluate model\u001b[39;00m\n\u001b[0;32m     72\u001b[0m pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(x)\n",
      "File \u001b[1;32mf:\\Simple-Auto-ML\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mf:\\Simple-Auto-ML\\.venv\\Lib\\site-packages\\optree\\ops.py:766\u001b[0m, in \u001b[0;36mtree_map\u001b[1;34m(func, tree, is_leaf, none_is_leaf, namespace, *rests)\u001b[0m\n\u001b[0;32m    764\u001b[0m leaves, treespec \u001b[38;5;241m=\u001b[39m _C\u001b[38;5;241m.\u001b[39mflatten(tree, is_leaf, none_is_leaf, namespace)\n\u001b[0;32m    765\u001b[0m flat_args \u001b[38;5;241m=\u001b[39m [leaves] \u001b[38;5;241m+\u001b[39m [treespec\u001b[38;5;241m.\u001b[39mflatten_up_to(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rests]\n\u001b[1;32m--> 766\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtreespec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflat_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid dtype: object"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from scipy.stats import zscore\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "df = pd.read_csv(\"https://data.heatonresearch.com/data/t81-558/crx.csv\", na_values=['NA', '?'])\n",
    "x_fields = []\n",
    "df[\"a1\"] = df[\"a1\"].fillna(df[\"a1\"].mode()[0])\n",
    "dummies = pd.get_dummies(df[\"a1\"], prefix=\"a1\", drop_first=True)\n",
    "df = pd.concat([df,dummies],axis=1)\n",
    "x_fields += dummies.columns.tolist()\n",
    "df[\"a2\"] = df[\"a2\"].fillna(df[\"a2\"].median())\n",
    "df[\"a2\"] = MinMaxScaler().fit_transform(df[['a2']])\n",
    "x_fields.append(\"a2\")\n",
    "df[\"s3\"] = MinMaxScaler().fit_transform(df[['s3']])\n",
    "x_fields.append(\"s3\")\n",
    "df[\"a4\"] = df[\"a4\"].fillna(df[\"a4\"].mode()[0])\n",
    "dummies = pd.get_dummies(df[\"a4\"], prefix=\"a4\", drop_first=True)\n",
    "df = pd.concat([df,dummies],axis=1)\n",
    "x_fields += dummies.columns.tolist()\n",
    "df[\"a5\"] = df[\"a5\"].fillna(df[\"a5\"].mode()[0])\n",
    "dummies = pd.get_dummies(df[\"a5\"], prefix=\"a5\", drop_first=True)\n",
    "df = pd.concat([df,dummies],axis=1)\n",
    "x_fields += dummies.columns.tolist()\n",
    "df[\"a6\"] = df[\"a6\"].fillna(df[\"a6\"].mode()[0])\n",
    "dummies = pd.get_dummies(df[\"a6\"], prefix=\"a6\", drop_first=True)\n",
    "df = pd.concat([df,dummies],axis=1)\n",
    "x_fields += dummies.columns.tolist()\n",
    "df[\"a7\"] = df[\"a7\"].fillna(df[\"a7\"].mode()[0])\n",
    "dummies = pd.get_dummies(df[\"a7\"], prefix=\"a7\", drop_first=True)\n",
    "df = pd.concat([df,dummies],axis=1)\n",
    "x_fields += dummies.columns.tolist()\n",
    "df[\"a8\"] = MinMaxScaler().fit_transform(df[['a8']])\n",
    "x_fields.append(\"a8\")\n",
    "dummies = pd.get_dummies(df[\"a9\"], prefix=\"a9\", drop_first=True)\n",
    "df = pd.concat([df,dummies],axis=1)\n",
    "x_fields += dummies.columns.tolist()\n",
    "dummies = pd.get_dummies(df[\"a10\"], prefix=\"a10\", drop_first=True)\n",
    "df = pd.concat([df,dummies],axis=1)\n",
    "x_fields += dummies.columns.tolist()\n",
    "df[\"a11\"] = MinMaxScaler().fit_transform(df[['a11']])\n",
    "x_fields.append(\"a11\")\n",
    "dummies = pd.get_dummies(df[\"a12\"], prefix=\"a12\", drop_first=True)\n",
    "df = pd.concat([df,dummies],axis=1)\n",
    "x_fields += dummies.columns.tolist()\n",
    "dummies = pd.get_dummies(df[\"a13\"], prefix=\"a13\", drop_first=True)\n",
    "df = pd.concat([df,dummies],axis=1)\n",
    "x_fields += dummies.columns.tolist()\n",
    "df[\"a14\"] = df[\"a14\"].fillna(df[\"a14\"].median())\n",
    "df[\"a14\"] = MinMaxScaler().fit_transform(df[['a14']])\n",
    "x_fields.append(\"a14\")\n",
    "df[\"a15\"] = MinMaxScaler().fit_transform(df[['a15']])\n",
    "x_fields.append(\"a15\")\n",
    "x = df[x_fields].values\n",
    "df[\"a16\"] = (df[\"a16\"]==\"+\").astype(int)\n",
    "y = df.a16.values\n",
    "# Construct model\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=x.shape[1], activation=\"relu\"))\n",
    "model.add(Dense(25, activation=\"relu\"))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
    "# Train model\n",
    "model.fit(x, y, verbose=2, epochs=100)\n",
    "# Evaluate model\n",
    "pred = model.predict(x)\n",
    "predict_classes = np.argmax(pred, axis=1)\n",
    "correct = accuracy_score(y, predict_classes)\n",
    "print(f\"Accuracy: {correct}\")\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=1)\n",
    "score = metrics.auc(fpr, tpr)\n",
    "print(f\"Area Under Curve: {score}\")\n",
    "score = metrics.log_loss(y, pred, eps=1e-07)\n",
    "print(f\"Log loss: {score}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
